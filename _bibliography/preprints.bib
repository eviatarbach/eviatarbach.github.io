@misc{bach_learning_2025,
  title = {Learning {{Optimal Filters Using Variational Inference}}},
  author = {Bach*, Eviatar and Baptista, Ricardo and Luk, Enoch and Stuart, Andrew},
  year = {2025},
  month = mar,
  number = {arXiv:2406.18066},
  eprint = {2406.18066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.18066},
  urldate = {2025-03-26},
  abstract = {Filtering - the task of estimating the conditional distribution for states of a dynamical system given partial and noisy observations - is important in many areas of science and engineering, including weather and climate prediction. However, the filtering distribution is generally intractable to obtain for high-dimensional, nonlinear systems. Filters used in practice, such as the ensemble Kalman filter (EnKF), provide biased probabilistic estimates for nonlinear systems and have numerous tuning parameters. Here, we present a framework for learning a parameterized analysis map - the transformation that takes samples from a forecast distribution, and combines with an observation, to update the approximate filtering distribution - using variational inference. In principle this can lead to a better approximation of the filtering distribution, and hence smaller bias. We show that this methodology can be used to learn the gain matrix, in an affine analysis map, for filtering linear and nonlinear dynamical systems; we also study the learning of inflation and localization parameters for an EnKF. The framework developed here can also be used to learn new filtering algorithms with more general forms for the analysis map.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems},
}

@misc{bach_inverse_2024,
  title = {Inverse {{Problems}} and {{Data Assimilation}}: {{A Machine Learning Approach}}},
  shorttitle = {Inverse {{Problems}} and {{Data Assimilation}}},
  author = {Bach*, Eviatar and Baptista, Ricardo and {Sanz-Alonso}, Daniel and Stuart, Andrew},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10523},
  eprint = {2410.10523},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10523},
  urldate = {2024-10-15},
  archiveprefix = {arXiv},
  doi = {10.48550/arXiv.2410.10523},
  abstract = {The aim of these notes is to demonstrate the potential for ideas in machine learning to impact on the fields of inverse problems and data assimilation. The perspective is one that is primarily aimed at researchers from inverse problems and/or data assimilation who wish to see a mathematical presentation of machine learning as it pertains to their fields. As a by-product, we include a succinct mathematical treatment of various topics in machine learning.}
}

@misc{bachLearningEnhancedEnsemble2025,
  title = {Learning {{Enhanced Ensemble Filters}}},
  author = {Bach, Eviatar and Baptista, Ricardo and Calvello, Edoardo and Chen, Bohan and Stuart, Andrew},
  year = {2025},
  month = apr,
  number = {arXiv:2504.17836},
  eprint = {2504.17836},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17836},
  urldate = {2025-04-28},
  abstract = {The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state--observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed by approximating the mean-field evolution using a novel form of neural operator taking probability distributions as input: a Measure Neural Mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-fieldlimit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. The derivation of methods from a mean-field formulation allows a single parameterization of the algorithm to be deployed at different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root-mean-square-error performance relative to leading methods in filtering the Lorenz 96 and Kuramoto-Sivashinsky models.},
  archiveprefix = {arXiv},
}
