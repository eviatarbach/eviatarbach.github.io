@misc{luk_learning_2024,
  title = {Learning {{Optimal Filters Using Variational Inference}}},
  author = {Luk, Enoch and Bach*, Eviatar and Baptista, Ricardo and Stuart, Andrew},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18066},
  eprint = {2406.18066},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-06-27},
  archiveprefix = {arXiv},
  doi = {10.48550/arxiv.2406.18066},
  abstract = {Filtering – the task of estimating the conditional distribution of states of a dynamical system given partial, noisy, observations – is important in many areas of science and engineering, including weather and climate prediction. However, the filtering distribution is generally intractable to obtain for high-dimensional, nonlinear systems. Filters used in practice, such as the ensemble Kalman filter (EnKF), are biased for nonlinear systems and have numerous tuning parameters. Here, we present a framework for learning a parameterized analysis map – the map that takes a forecast distribution and observations to the filtering distribution – using variational inference. We show that this methodology can be used to learn gain matrices for filtering linear and nonlinear dynamical systems, as well as inflation and localization parameters for an EnKF. Future work will apply this framework to learn new filtering algorithms.},
}

@misc{bach_inverse_2024,
  title = {Inverse {{Problems}} and {{Data Assimilation}}: {{A Machine Learning Approach}}},
  shorttitle = {Inverse {{Problems}} and {{Data Assimilation}}},
  author = {Bach*, Eviatar and Baptista, Ricardo and {Sanz-Alonso}, Daniel and Stuart, Andrew},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10523},
  eprint = {2410.10523},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10523},
  urldate = {2024-10-15},
  archiveprefix = {arXiv},
  doi = {10.48550/arXiv.2410.10523},
  abstract = {The aim of these notes is to demonstrate the potential for ideas in machine learning to impact on the fields of inverse problems and data assimilation. The perspective is one that is primarily aimed at researchers from inverse problems and/or data assimilation who wish to see a mathematical presentation of machine learning as it pertains to their fields. As a by-product, we include a succinct mathematical treatment of various topics in machine learning.}
}

@misc{bach_forecast_2024,
  title = {Forecast Error Growth: {{A}} Dynamic-Stochastic Model},
  shorttitle = {Forecast Error Growth},
  author = {Bach*, Eviatar and Crisan, Dan and Ghil, Michael},
  year = {2024},
  month = nov,
  number = {arXiv:2411.06623},
  eprint = {2411.06623},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.06623},
  urldate = {2024-11-12},
  archiveprefix = {arXiv},
  abstract = {There is a history of simple forecast error growth models designed to capture the key properties of error growth in operational numerical weather prediction (NWP) models. We propose here such a scalar model that relies on the previous ones and incorporates multiplicative noise in a nonlinear stochastic differential equation (SDE). We analyze the properties of the SDE, including the shape of the error growth curve for small times and its stationary distribution, and prove well-posedness and positivity of solutions. We then fit this model to operational NWP error growth curves, showing good agreement with both the mean and probabilistic features of the error growth. These results suggest that the dynamic-stochastic error growth model proposed herein and similar ones could play a role in many other areas of the sciences that involve prediction.}
}

@misc{vernon_nesterov_2025,
  title = {Nesterov {{Acceleration}} for {{Ensemble Kalman Inversion}} and {{Variants}}},
  author = {Vernon, Sydney and Bach*, Eviatar and Dunbar, Oliver R. A.},
  year = {2025},
  month = jan,
  number = {arXiv:2501.08779},
  eprint = {2501.08779},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.08779},
  urldate = {2025-01-16},
  archiveprefix = {arXiv},
  abstract = {Ensemble Kalman inversion (EKI) is a derivative-free, particle-based optimization method for solving inverse problems. It can be shown that EKI approximates a gradient flow, which allows the application of methods for accelerating gradient descent. Here, we show that Nesterov acceleration is effective in speeding up the reduction of the EKI cost function on a variety of inverse problems. We also implement Nesterov acceleration for two EKI variants, unscented Kalman inversion and ensemble transform Kalman inversion. Our specific implementation takes the form of a particle-level nudge that is demonstrably simple to couple in a black-box fashion with any existing EKI variant algorithms, comes with no additional computational expense, and with no additional tuning hyperparameters. This work shows a pathway for future research to translate advances in gradient-based optimization into advances in gradient-free Kalman optimization.}
}
